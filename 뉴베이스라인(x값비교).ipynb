{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('raw_data_20180811.pickle','rb') as handle:\n",
    "\n",
    "    raw_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "kospi200 = pd.read_excel(\"C:\\\\Users\\\\axasd\\\\Downloads\\\\상장법인목록.xlsx\")\n",
    "kospi200 = kospi200[kospi200.columns[:2]]\n",
    "kospi200[\"종목코드\"] = kospi200[\"종목코드\"].map('{:06d}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dict = {\n",
    "    i : j for i, j in zip(kospi200[\"회사명\"], kospi200[\"종목코드\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in raw_data.keys():\n",
    "    raw_data[i] = raw_data[i].assign(sum_close_3 = raw_data[i].Close.rolling(window = 3).sum())\n",
    "    raw_data[i] = raw_data[i].assign(sum_close_5 = raw_data[i].Close.rolling(window =5).sum())\n",
    "    raw_data[i] = raw_data[i].assign(ma_5 = raw_data[i].Close.rolling(window = 5).mean())\n",
    "    raw_data[i] = raw_data[i].assign(ma_10 = raw_data[i].Close.rolling(window = 10).mean())\n",
    "    \n",
    "for i in raw_data.keys():\n",
    "    raw_data[i] = raw_data[i].assign(pct_change = raw_data[i].Close.pct_change().fillna(0))\n",
    "    raw_data[i] = raw_data[i].assign(Y = np.where(raw_data[i][\"pct_change\"].fillna(0)>=0, 1,0))\n",
    "    \n",
    "for i in raw_data.keys():\n",
    "    raw_data[i] = raw_data[i].assign(pct_change2 = raw_data[i].sum_close_3.pct_change().fillna(0))\n",
    "    raw_data[i] = raw_data[i].assign(pct_change3 = raw_data[i].sum_close_5.pct_change().fillna(0))\n",
    "    raw_data[i] = raw_data[i].assign(sencond_Y = np.where(raw_data[i][\"pct_change2\"].fillna(0)>=0, 1,0))\n",
    "    raw_data[i] = raw_data[i].assign(third_Y = np.where(raw_data[i][\"pct_change3\"].fillna(0)>=0, 1,0))\n",
    "    \n",
    "for i in raw_data.keys():\n",
    "    raw_data[i] = raw_data[i].dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_keys = kospi200[\"종목코드\"]\n",
    "for i in copy_keys:\n",
    "    if (len(raw_data[i]) < 252):\n",
    "        del raw_data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = {}\n",
    "train_idx1 = {}\n",
    "test1 = {}\n",
    "test_idx1 = {}\n",
    "for code, df in raw_data.items():\n",
    "    train1[code] = df.iloc[:int(len(df)*0.8)][[\"Open\"]]\n",
    "    train_idx1[code] = df.iloc[:int(len(df)*0.8)][\"third_Y\"]\n",
    "    test1[code] = df.iloc[int(len(df)*0.8):][[\"Open\"]]\n",
    "    test_idx1[code] = df.iloc[int(len(df)*0.8):][\"third_Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = {}\n",
    "train_idx2 = {}\n",
    "test2 = {}\n",
    "test_idx2 = {}\n",
    "for code, df in raw_data.items():\n",
    "    train2[code] = df.iloc[:int(len(df)*0.8)][[\"ma_5\"]]\n",
    "    train_idx2[code] = df.iloc[:int(len(df)*0.8)][\"third_Y\"]\n",
    "    test2[code] = df.iloc[int(len(df)*0.8):][[\"ma_5\"]]\n",
    "    test_idx2[code] = df.iloc[int(len(df)*0.8):][\"third_Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = {}\n",
    "train_idx3 = {}\n",
    "test3 = {}\n",
    "test_idx3 = {}\n",
    "for code, df in raw_data.items():\n",
    "    train3[code] = df.iloc[:int(len(df)*0.8)][[\"Open\",\"ma_5\"]]\n",
    "    train_idx3[code] = df.iloc[:int(len(df)*0.8)][\"third_Y\"]\n",
    "    test3[code] = df.iloc[int(len(df)*0.8):][[\"Open\",\"ma_5\"]]\n",
    "    test_idx3[code] = df.iloc[int(len(df)*0.8):][\"third_Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4 = {}\n",
    "train_idx4 = {}\n",
    "test4 = {}\n",
    "test_idx4 = {}\n",
    "for code, df in raw_data.items():\n",
    "    train4[code] = df.iloc[:int(len(df)*0.8)][[\"ma_5\",\"ma_10\"]]\n",
    "    train_idx4[code] = df.iloc[:int(len(df)*0.8)][\"third_Y\"]\n",
    "    test4[code] = df.iloc[int(len(df)*0.8):][[\"ma_5\",\"ma_10\"]]\n",
    "    test_idx4[code] = df.iloc[int(len(df)*0.8):][\"third_Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5 = {}\n",
    "train_idx5 = {}\n",
    "test5 = {}\n",
    "test_idx5 = {}\n",
    "for code, df in raw_data.items():\n",
    "    train5[code] = df.iloc[:int(len(df)*0.8)][[\"Open\",\"ma_5\",\"ma_10\"]]\n",
    "    train_idx5[code] = df.iloc[:int(len(df)*0.8)][\"third_Y\"]\n",
    "    test5[code] = df.iloc[int(len(df)*0.8):][[\"Open\",\"ma_5\",\"ma_10\"]]\n",
    "    test_idx5[code] = df.iloc[int(len(df)*0.8):][\"third_Y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "li1 = [\"Open\"]\n",
    "for code in raw_data.keys():\n",
    "    train1[code][li1] = std_scaler.fit_transform(train1[code][li1].values)\n",
    "    test1[code][li1] = std_scaler.fit_transform(test1[code][li1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "li2 = [\"ma_5\"]\n",
    "for code in raw_data.keys():\n",
    "    train2[code][li2] = std_scaler.fit_transform(train2[code][li2].values)\n",
    "    test2[code][li2] = std_scaler.fit_transform(test2[code][li2].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "li3 = [\"Open\",\"ma_5\"]\n",
    "for code in raw_data.keys():\n",
    "    train3[code][li3] = std_scaler.fit_transform(train3[code][li3].values)\n",
    "    test3[code][li3] = std_scaler.fit_transform(test3[code][li3].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "li4 = [\"ma_5\",\"ma_10\"]\n",
    "for code in raw_data.keys():\n",
    "    train4[code][li4] = std_scaler.fit_transform(train4[code][li4].values)\n",
    "    test4[code][li4] = std_scaler.fit_transform(test4[code][li4].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "li5 = [\"Open\",\"ma_5\",\"ma_10\"]\n",
    "for code in raw_data.keys():\n",
    "    train5[code][li5] = std_scaler.fit_transform(train5[code][li5].values)\n",
    "    test5[code][li5] = std_scaler.fit_transform(test5[code][li5].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "DTC_clf = DecisionTreeClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_list_1 = pd.DataFrame(columns=('LR','DT','SVM'))\n",
    "\n",
    "for i in raw_data.keys():\n",
    "    a1=[]\n",
    "    X_train1 = train1[i].values\n",
    "    y_train1 = train_idx1[i].values\n",
    "    X_test1 = test1[i].values\n",
    "    y_test1 = test_idx1[i].values\n",
    "    for clf in (log_clf, DTC_clf, svm_clf):\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        y_pred1 = clf.predict(X_test1)\n",
    "        a1.append(accuracy_score(y_test1, y_pred1))\n",
    "    score_list_1.loc[i] = a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_list_2 = pd.DataFrame(columns=('LR','DT','SVM'))\n",
    "\n",
    "for i in raw_data.keys():\n",
    "    a2=[]\n",
    "    X_train2 = train2[i].values\n",
    "    y_train2 = train_idx2[i].values\n",
    "    X_test2 = test2[i].values\n",
    "    y_test2 = test_idx2[i].values\n",
    "    for clf in (log_clf, DTC_clf, svm_clf):\n",
    "        clf.fit(X_train2, y_train2)\n",
    "        y_pred2 = clf.predict(X_test2)\n",
    "        a2.append(accuracy_score(y_test2, y_pred2))\n",
    "    score_list_2.loc[i] = a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_list_3 = pd.DataFrame(columns=('LR','DT','SVM'))\n",
    "\n",
    "for i in raw_data.keys():\n",
    "    a3=[]\n",
    "    X_train3 = train3[i].values\n",
    "    y_train3 = train_idx3[i].values\n",
    "    X_test3 = test3[i].values\n",
    "    y_test3 = test_idx3[i].values\n",
    "    for clf in (log_clf, DTC_clf, svm_clf):\n",
    "        clf.fit(X_train3, y_train3)\n",
    "        y_pred3 = clf.predict(X_test3)\n",
    "        a3.append(accuracy_score(y_test3, y_pred3))\n",
    "    score_list_3.loc[i] = a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_list_4 = pd.DataFrame(columns=('LR','DT','SVM'))\n",
    "\n",
    "for i in raw_data.keys():\n",
    "    a4=[]\n",
    "    X_train4 = train4[i].values\n",
    "    y_train4 = train_idx4[i].values\n",
    "    X_test4 = test4[i].values\n",
    "    y_test4 = test_idx4[i].values\n",
    "    for clf in (log_clf, DTC_clf, svm_clf):\n",
    "        clf.fit(X_train4, y_train4)\n",
    "        y_pred4 = clf.predict(X_test4)\n",
    "        a4.append(accuracy_score(y_test4, y_pred4))\n",
    "    score_list_4.loc[i] = a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_list_5 = pd.DataFrame(columns=('LR','DT','SVM'))\n",
    "\n",
    "for i in raw_data.keys():\n",
    "    a5=[]\n",
    "    X_train5 = train5[i].values\n",
    "    y_train5 = train_idx5[i].values\n",
    "    X_test5 = test5[i].values\n",
    "    y_test5 = test_idx5[i].values\n",
    "    for clf in (log_clf, DTC_clf, svm_clf):\n",
    "        clf.fit(X_train5, y_train5)\n",
    "        y_pred5 = clf.predict(X_test5)\n",
    "        a5.append(accuracy_score(y_test5, y_pred5))\n",
    "    score_list_5.loc[i] = a5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature : open\n",
      "               LR          DT         SVM\n",
      "count  197.000000  197.000000  197.000000\n",
      "mean     0.518685    0.505691    0.512399\n",
      "std      0.052390    0.037905    0.046066\n",
      "min      0.365325    0.300310    0.386997\n",
      "25%      0.482972    0.486068    0.486068\n",
      "50%      0.523220    0.507740    0.510836\n",
      "75%      0.554180    0.529412    0.541796\n",
      "max      0.715170    0.606811    0.671827\n",
      "feature : MA5\n",
      "               LR          DT         SVM\n",
      "count  197.000000  197.000000  197.000000\n",
      "mean     0.497324    0.501337    0.501036\n",
      "std      0.048207    0.031478    0.043638\n",
      "min      0.291022    0.391753    0.386997\n",
      "25%      0.467492    0.480702    0.473684\n",
      "50%      0.497326    0.501548    0.504644\n",
      "75%      0.530769    0.520124    0.526316\n",
      "max      0.656934    0.702786    0.687307\n",
      "feature : open, MA5\n",
      "               LR          DT         SVM\n",
      "count  197.000000  197.000000  197.000000\n",
      "mean     0.678308    0.611466    0.632371\n",
      "std      0.042324    0.036300    0.066762\n",
      "min      0.463415    0.495356    0.390093\n",
      "25%      0.653251    0.591331    0.603715\n",
      "50%      0.681115    0.613003    0.643963\n",
      "75%      0.706161    0.631579    0.681115\n",
      "max      0.793103    0.752322    0.727554\n",
      "feature : MA5, MA10\n",
      "               LR          DT         SVM\n",
      "count  197.000000  197.000000  197.000000\n",
      "mean     0.734438    0.677576    0.689253\n",
      "std      0.048809    0.042169    0.069703\n",
      "min      0.331269    0.541463    0.393189\n",
      "25%      0.721362    0.650155    0.662539\n",
      "50%      0.739938    0.681115    0.708978\n",
      "75%      0.761610    0.705882    0.733746\n",
      "max      0.801858    0.789474    0.808050\n",
      "feature : open,MA5, MA10\n",
      "               LR          DT         SVM\n",
      "count  197.000000  197.000000  197.000000\n",
      "mean     0.771839    0.705898    0.739979\n",
      "std      0.038494    0.043598    0.060694\n",
      "min      0.502439    0.525424    0.448780\n",
      "25%      0.752322    0.678019    0.718266\n",
      "50%      0.777090    0.712074    0.755418\n",
      "75%      0.795666    0.736842    0.780186\n",
      "max      0.839009    0.783282    0.842105\n"
     ]
    }
   ],
   "source": [
    "print('feature : open')\n",
    "print(score_list_1.describe())\n",
    "print('feature : MA5')\n",
    "print(score_list_2.describe())\n",
    "print('feature : open, MA5')\n",
    "print(score_list_3.describe())\n",
    "print('feature : MA5, MA10')\n",
    "print(score_list_4.describe())\n",
    "print('feature : open,MA5, MA10')\n",
    "print(score_list_5.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MA간의 비교를 할때, 모델성능이 크게 좋아졌으며, open과 MA간의 비교또한 모델성능을 향상시키는데 유의미한 요소였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
